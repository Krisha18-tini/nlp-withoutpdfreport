# -*- coding: utf-8 -*-
"""MOST_LATEST_1_new_updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ElXXQ4i9Et3X2RS6pF-01d1XB5CB75WB
"""

import streamlit as st
import pandas as pd
import numpy as np
import re
import json
import spacy
from openai import OpenAI
import time
from collections.abc import Iterable
import plotly.express as px
import plotly.graph_objects as go
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
import io
import os
from pathlib import Path

# Optional PDF dependencies
try:
    from reportlab.lib.pagesizes import A4
    from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle, PageBreak
    from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
    from reportlab.lib.units import inch
    from reportlab.lib import colors
    import plotly.io as pio
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="NLP Requirements Analysis Tool",
    page_icon="üî¨",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .section-header {
        font-size: 1.8rem;
        color: #2e8b57;
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'processed_df' not in st.session_state:
    st.session_state.processed_df = None
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'original_df' not in st.session_state:
    st.session_state.original_df = None
if 'selected_discipline' not in st.session_state:
    st.session_state.selected_discipline = None
if 'selected_document_type' not in st.session_state:
    st.session_state.selected_document_type = None
if 'uploaded_filename' not in st.session_state:
    st.session_state.uploaded_filename = None
if 'speed_option' not in st.session_state:
    st.session_state.speed_option = None

# Load spaCy model
@st.cache_resource
def load_spacy_model():
    try:
        return spacy.load("en_core_web_sm")
    except OSError:
        st.error("spaCy English model not found. Please install it using: python -m spacy download en_core_web_sm")
        return None

# Helper function to get filename without extension
def get_filename_without_extension(filename):
    return Path(filename).stem

# Smart column detection function
def detect_columns(df):
    """
    Smart detection of Numbering and Text columns based on header names
    Returns: (numbering_col, text_col, detection_info)
    """
    columns = df.columns.tolist()

    # Patterns for numbering column (case-insensitive)
    numbering_patterns = [
        'numbering', 'number', 'num', 'id', 'req id', 'req_id', 'reqid',
        'item', 'index', 'sr no', 'sr_no', 'serial', 'sequence', 'ref'
    ]

    # Patterns for text column (case-insensitive)
    text_patterns = [
        'text', 'requirement', 'requirements', 'description', 'requirement text',
        'requirements text', 'requirement description', 'requirements description',
        'req text', 'req description', 'details', 'specification', 'content'
    ]

    numbering_col = None
    text_col = None

    # Search for numbering column
    for col in columns:
        col_lower = col.lower().strip()
        for pattern in numbering_patterns:
            if pattern in col_lower or col_lower == pattern:
                numbering_col = col
                break
        if numbering_col:
            break

    # Search for text column
    for col in columns:
        col_lower = col.lower().strip()
        for pattern in text_patterns:
            if pattern in col_lower or col_lower == pattern:
                text_col = col
                break
        if text_col:
            break

    # Create detection info
    detection_info = {
        'total_columns': len(columns),
        'all_columns': columns,
        'numbering_found': numbering_col is not None,
        'text_found': text_col is not None,
        'numbering_col': numbering_col,
        'text_col': text_col
    }

    return numbering_col, text_col, detection_info

def prepare_dataframe_with_smart_detection(uploaded_file):
    """
    Read file and apply smart column detection
    Returns: (df, detection_info, error_message)
    """
    try:
        # Read file
        if uploaded_file.name.endswith('.csv'):
            df_full = pd.read_csv(uploaded_file, encoding='ISO-8859-1')
            file_type = "CSV"
        elif uploaded_file.name.endswith(('.xlsx', '.xls')):
            # Always read first sheet (index 0)
            df_full = pd.read_excel(uploaded_file, sheet_name=0)
            excel_file = pd.ExcelFile(uploaded_file)
            sheet_name = excel_file.sheet_names[0]
            file_type = f"Excel (Sheet: {sheet_name})"
        else:
            return None, None, "Unsupported file format"

        # Apply smart column detection
        numbering_col, text_col, detection_info = detect_columns(df_full)

        # Prepare final dataframe
        if numbering_col and text_col:
            # Smart detection successful
            df = df_full[[numbering_col, text_col]].copy()
            df.columns = ['Numbering', 'Text']  # Standardize column names
            detection_info['method'] = 'smart_detection'
            detection_info['file_type'] = file_type

        elif len(df_full.columns) >= 2:
            # Fallback to first two columns
            df = df_full.iloc[:, :2].copy()
            df.columns = ['Numbering', 'Text']  # Standardize column names
            detection_info['method'] = 'fallback_position'
            detection_info['file_type'] = file_type
            detection_info['warning'] = f"Could not find expected column names. Using first two columns: '{df_full.columns[0]}' and '{df_full.columns[1]}'"

        else:
            return None, detection_info, "File must have at least 2 columns"

        return df, detection_info, None

    except Exception as e:
        return None, None, f"Error reading file: {str(e)}"

# Text cleaning function
def clean_text(text):
    if pd.isna(text):
        return ""
    text = re.sub(r"^\s*[\u2160-\u217F]+[.)]?\s*", "", text)
    text = re.sub(r"^\s*[\(\[]?[ivxlcdmIVXLCDM]+\)?[.\)]\s*", "", text)
    text = re.sub(r"^\s*[a-zA-Z]+\)?[.\)]\s*", "", text)
    text = re.sub(r"^\s*\d+(\.\d+)*[.)]?\s*", "", text)
    text = re.sub(r"[‚Ä¢‚Üí\-‚ñ∫‚óÜ‚óè]", " ", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

# NLP extraction constants
SUBJECTS = {"nsubj", "nsubjpass", "csubj", "csubjpass", "agent", "expl"}
OBJECTS = {"dobj", "dative", "attr", "oprd"}
BREAKER_POS = {"CCONJ", "VERB"}
NEGATIONS = {"no", "not", "n't", "never", "none"}

def _is_passive(tokens):
    return any(tok.dep_ == "auxpass" for tok in tokens)

def _is_negated(tok):
    return any(dep.lower_ in NEGATIONS for dep in list(tok.lefts) + list(tok.rights))

def contains_conj(depSet):
    return any(conj in depSet for conj in ["and", "or", "nor", "but", "yet", "so", "for"])

def _get_objs_from_conjunctions(objs):
    more_objs = []
    for obj in objs:
        rights = list(obj.rights)
        rightDeps = {tok.lower_ for tok in rights}
        if contains_conj(rightDeps):
            more_objs.extend([tok for tok in rights if tok.dep_ in OBJECTS or tok.pos_ == "NOUN"])
    return more_objs

def _get_subs_from_conjunctions(subs):
    more_subs = []
    for sub in subs:
        rights = list(sub.rights)
        rightDeps = {tok.lower_ for tok in rights}
        if contains_conj(rightDeps):
            more_subs.extend([tok for tok in rights if tok.dep_ in SUBJECTS or tok.pos_ == "NOUN"])
    return more_subs

def _get_all_subs(v):
    verb_negated = _is_negated(v)
    subs = [tok for tok in v.lefts if tok.dep_ in SUBJECTS and tok.pos_ != "DET"]
    if subs:
        subs.extend(_get_subs_from_conjunctions(subs))
    return subs, verb_negated

def _get_objs_from_prepositions(deps, is_pas):
    objs = []
    for dep in deps:
        if dep.pos_ == "ADP" and (dep.dep_ == "prep" or (is_pas and dep.dep_ == "agent")):
            objs.extend([tok for tok in dep.rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')])
    return objs

def _get_obj_from_xcomp(deps, is_pas):
    for dep in deps:
        if dep.pos_ == "VERB" and dep.dep_ == "xcomp":
            v = dep
            rights = list(v.rights)
            objs = [tok for tok in rights if tok.dep_ in OBJECTS]
            objs.extend(_get_objs_from_prepositions(rights, is_pas))
            if objs:
                return v, objs
    return None, None

def _find_verbs(tokens):
    verbs = [tok for tok in tokens if tok.pos_ == "VERB" and tok.dep_ not in ["aux", "auxpass"]]
    if not verbs:
        verbs = [tok for tok in tokens if tok.pos_ in ["VERB", "AUX"]]
    return verbs

def expand(item, tokens, visited):
    parts = []
    action_verbs = {"shall", "must", "should", "will"}

    for part in item.lefts:
        if part.pos_ in BREAKER_POS or part.lower_ in action_verbs:
            break
        if part.lower_ not in NEGATIONS:
            parts.append(part)

    parts.append(item)

    for part in item.rights:
        if part.pos_ in BREAKER_POS or part.lower_ in action_verbs:
            break
        if part.lower_ not in NEGATIONS:
            parts.append(part)

    if hasattr(parts[-1], 'rights'):
        for item2 in parts[-1].rights:
            if item2.pos_ in {"DET", "NOUN"} and item2.i not in visited:
                visited.add(item2.i)
                parts.extend(expand(item2, tokens, visited))
            break

    return parts

def to_str(tokens):
    if isinstance(tokens, Iterable):
        return ' '.join([item.text for item in tokens])
    return ''

def extract_enhanced_roles(text, nlp):
    doc = nlp(text)
    action_verb = None
    subject = None
    verb = None
    obj = None
    constraint = None
    visited = set()

    # Case-insensitive action verb detection
    for token in doc:
        if token.lower_ in ['shall', 'must', 'should', 'will']:
            action_verb = token.lower_.capitalize()  # Normalize case
            break

    if action_verb:
        tokens = list(doc)
        is_pas = _is_passive(tokens)
        verbs = _find_verbs(tokens)

        for v in verbs:
            subs, _ = _get_all_subs(v)
            v_final, objs = _get_obj_from_xcomp(list(v.rights), is_pas)
            if v_final is None:
                v_final = v
                objs = [tok for tok in v.rights if tok.dep_ in OBJECTS or (is_pas and tok.dep_ == 'pobj')]
                objs.extend(_get_objs_from_prepositions(list(v.rights), is_pas))
                objs.extend(_get_objs_from_conjunctions(objs))

            if subs:
                subject = to_str(expand(subs[0], tokens, visited))
            if objs:
                obj = to_str(expand(objs[0], tokens, visited))
            if v_final:
                verb = v_final.lemma_
            break

        constraint_pattern = r"\b\d+(\.\d+)?\s?(seconds|minutes|hours|¬∞C|¬∞F|kg|g|bar|psi|m|cm|mm|%)\b"
        match = re.search(constraint_pattern, text)
        if match:
            constraint = match.group()

    return {
        "action_verb": action_verb,
        "subject": subject,
        "verb": verb,
        "object": obj,
        "constraint": constraint
    }

# Parallel processing functions
def process_refinement_batch(client, batch_data):
    results = []
    for item in batch_data:
        try:
            if len(item) == 5:
                idx, subject, obj, verb, text = item
            else:
                # Handle different tuple sizes
                idx = item[0]
                subject = item[1] if len(item) > 1 else None
                obj = item[2] if len(item) > 2 else None
                verb = item[3] if len(item) > 3 else None
                text = item[4] if len(item) > 4 else None

            result = refine_with_descriptors(client, subject, obj, verb, text)
            results.append((idx, result))
            time.sleep(0.02)
        except Exception as e:
            results.append((idx, {"subject_refined": subject if 'subject' in locals() else None, "verb_refined": verb if 'verb' in locals() else None, "object_refined": obj if 'obj' in locals() else None, "confidence_score": 0.0, "notes": f"Error: {str(e)}"}))
    return results

def process_constraint_batch(client, batch_data):
    results = []
    for idx, text in batch_data:
        try:
            result = identify_constraints_with_chatgpt(client, text)
            results.append((idx, result))
            time.sleep(0.02)
        except Exception as e:
            results.append((idx, {"constraints_identified": [], "constraint_types": [], "constraint_values": [], "constraint_count": 0, "confidence_score": 0.0, "notes": f"Error: {str(e)}"}))
    return results

# ChatGPT functions
def refine_with_descriptors(client, subject, obj, verb, original_text=None):
    if not subject and not obj and not verb:
        return {"subject_refined": None, "verb_refined": None, "object_refined": None, "confidence_score": 0.0, "notes": "No components to refine"}

    prompt = f"""
    You are an expert in engineering requirements analysis. Refine these extracted components:

    ORIGINAL: "{original_text or 'Not provided'}"

    COMPONENTS:
    - Subject: "{subject}"
    - Verb: "{verb}"
    - Object: "{obj}"

    Respond in JSON format:
    {{
        "subject_refined": "refined subject with qualifiers",
        "verb_refined": "refined verb with conditions",
        "object_refined": "refined object with specifications",
        "confidence_score": 0.95,
        "notes": "brief explanation"
    }}
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a technical writing expert."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=500
        )
        content = response.choices[0].message.content.strip()
        if content.startswith("```json"):
            content = content.replace("```json", "").replace("```", "").strip()
        elif content.startswith("```"):
            content = content.replace("```", "").strip()
        return json.loads(content)
    except Exception as e:
        return {"subject_refined": subject, "verb_refined": verb, "object_refined": obj, "confidence_score": 0.0, "notes": f"Error: {str(e)}"}

def identify_constraints_with_chatgpt(client, text):
    if not text or pd.isna(text):
        return {"constraints_identified": [], "constraint_types": [], "constraint_values": [], "constraint_count": 0, "confidence_score": 0.0, "notes": "No text provided"}

    prompt = f"""
    Extract constraints from this engineering requirement:

    TEXT: "{text}"

    Find: numerical limits, performance requirements, standards, material specs, operational conditions.

    Respond in JSON:
    {{
        "constraints_identified": ["constraint1", "constraint2"],
        "constraint_types": ["type1", "type2"],
        "constraint_values": ["value1", "value2"],
        "constraint_count": 2,
        "confidence_score": 0.9,
        "notes": "summary"
    }}
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Extract engineering constraints concisely."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.1,
            max_tokens=400
        )
        content = response.choices[0].message.content.strip()
        if content.startswith("```json"):
            content = content.replace("```json", "").replace("```", "").strip()
        elif content.startswith("```"):
            content = content.replace("```", "").strip()

        result = json.loads(content)
        if not isinstance(result.get("constraints_identified"), list):
            result["constraints_identified"] = []
        if not isinstance(result.get("constraint_types"), list):
            result["constraint_types"] = []
        if not isinstance(result.get("constraint_values"), list):
            result["constraint_values"] = []
        result["constraint_count"] = len(result["constraints_identified"])
        return result
    except Exception as e:
        return {"constraints_identified": [], "constraint_types": [], "constraint_values": [], "constraint_count": 0, "confidence_score": 0.0, "notes": f"Error: {str(e)}"}

def incose_compliance_with_gpt(client, text):
    if not text or pd.isna(text):
        return {"clarity_level": "Unknown", "issues": [], "likely_violated_rules": [], "improved_requirement": ""}

    # Enhanced INCOSE rules for comprehensive analysis
    incose_rules = """
R1: Structured Statements: Requirement statements must conform to an agreed pattern, resulting in a well-structured complete statement.
R2: Active Voice: Use the active voice with the responsible entity clearly as the subject.
R3: Appropriate Subject-Verb: Ensure the subject and verb are appropriate to the entity referenced.
R4: Defined Terms: Define all terms used within the requirement in a glossary or data dictionary.
R5: Definite Articles: Use "the" instead of "a" when referring to a specific item.
R6: Common Units of Measure: Always use consistent and explicit units for quantities.
R7: Vague Terms: Avoid vague terms and adjectives like "some", "any", "appropriate", "efficient".
R8: Escape Clauses: Avoid vague options like "as far as possible", "if practicable".
R9: Open-Ended Clauses: Avoid endings like "etc.", "and so on".
R10: Superfluous Infinitives: Avoid "to be able to", "to be intended to".
R11: Separate Clauses: Each condition should be in a separate clause.
R12: Correct Grammar: Use correct grammar.
R13: Correct Spelling: Ensure correct spelling.
R14: Correct Punctuation: Use proper punctuation.
R15: Logical Expressions: Use explicit logic terms (AND, OR, NOT).
R16: Use of NOT: Avoid the word "not".
R17: Use of Oblique Symbol: Avoid "/" in text.
R18: Single Thought Sentence: One requirement = one thought.
R19: Combinators: Avoid unnecessary combinators like "and/or".
R20: Purpose of Phrases: Ensure purpose phrases are explicit.
R21: Enumeration: Enumerate lists clearly.
R22: Supporting Diagram or Model: Include diagrams or ICD references if complex.
R23: Pronouns: Avoid personal and indefinite pronouns.
R24: Headings: Ensure headings clearly relate to requirement text.
R25: Absolutes: Avoid absolutes like "100%", "always", "never".
R26: Explicit Conditions: State conditions explicitly.
R27: Multiple Conditions: Express multiple conditions clearly.
R28: Classification: Use classification tags if relevant.
R29: Unique Expression: Avoid duplicate statements.
R30: Solution Free: Avoid design or implementation details unless justified.
R31: Universal Qualification: Use "each" instead of "all", "any".
R32: Range of Values: Provide explicit ranges for numerical values.
R33: Measurable Performance: Provide measurable acceptance criteria.
R34: Temporal Dependencies: Express time-based constraints explicitly.
R35: Consistent Terms and Units: Ensure consistent terminology and units.
R36: Acronyms: Define acronyms on first use.
R37: Abbreviations: Avoid undefined abbreviations.
R38: Style Guide: Follow a consistent style guide for format and tone.
R39: Decimal Format: Use standard decimal notation for numbers.
R40: Related Needs and Requirements: Group related requirements logically.
R41: Structured List: Organize sets of requirements in a clear, structured list.
"""

    prompt = f"""
    Analyze the following requirement for INCOSE compliance based on these rules:

    {incose_rules}

    Requirement: "{text}"

    Tasks:
    1. Rate clarity as "High", "Medium", or "Low" based on:
       - High: Clear, unambiguous, follows most INCOSE rules
       - Medium: Generally clear but has some issues
       - Low: Unclear, ambiguous, or has major issues

    2. List specific issues found in the requirement (be precise and actionable)

    3. Identify exactly which INCOSE rules are violated. For each violated rule, provide the full rule description exactly as given above (e.g., "R7: Vague Terms: Avoid vague terms and adjectives like 'some', 'any', 'appropriate', 'efficient'")

    4. Provide an improved version of the requirement that addresses all identified violations

    Important: Be thorough in identifying violated rules. Check against ALL 41 rules systematically.

    Respond ONLY in valid JSON format:
    {{
        "clarity_level": "High/Medium/Low",
        "issues": ["specific issue 1", "specific issue 2"],
        "likely_violated_rules": ["R7: Vague Terms: Avoid vague terms and adjectives like 'some', 'any', 'appropriate', 'efficient'", "R18: Single Thought Sentence: One requirement = one thought"],
        "improved_requirement": "Improved version here that addresses all violations"
    }}
    """

    # Retry logic for robustness
    for attempt in range(3):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "You are an expert INCOSE requirements analyst. Analyze requirements systematically against all 41 INCOSE rules and provide accurate compliance assessment."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=1000,
                timeout=30
            )

            content = response.choices[0].message.content.strip()

            # Clean up markdown formatting
            if content.startswith("```json"):
                content = content.replace("```json", "").replace("```", "").strip()
            elif content.startswith("```"):
                content = content.replace("```", "").strip()

            # Parse and validate JSON
            result = json.loads(content)

            # Validate required fields
            required_fields = ["clarity_level", "issues", "likely_violated_rules", "improved_requirement"]
            for field in required_fields:
                if field not in result:
                    result[field] = [] if field in ["issues", "likely_violated_rules"] else ""

            # Ensure lists are actually lists
            if not isinstance(result["issues"], list):
                result["issues"] = [str(result["issues"])] if result["issues"] else []
            if not isinstance(result["likely_violated_rules"], list):
                result["likely_violated_rules"] = [str(result["likely_violated_rules"])] if result["likely_violated_rules"] else []

            # Validate clarity level
            if result["clarity_level"] not in ["High", "Medium", "Low"]:
                result["clarity_level"] = "Medium"  # Default fallback

            return result

        except Exception as e:
            if attempt == 2:  # Last attempt
                return {
                    "clarity_level": "Unknown",
                    "issues": [f"Analysis failed after {attempt + 1} attempts: {str(e)}"],
                    "likely_violated_rules": [],
                    "improved_requirement": ""
                }
            time.sleep(1)  # Brief pause before retry

    return {"clarity_level": "Unknown", "issues": [], "likely_violated_rules": [], "improved_requirement": ""}

# Main processing function with corrected logic
def process_dataframe(df, api_key, discipline, document_type, progress_callback=None, use_parallel=True):
    client = OpenAI(api_key=api_key)
    nlp = load_spacy_model()

    if nlp is None:
        st.error("Cannot proceed without spaCy model")
        return None

    # Stage 1: Text cleaning
    if progress_callback:
        progress_callback(0.05, "Cleaning text...")
    df['clean_text'] = df.iloc[:, 1].astype(str).apply(clean_text)
    df = df[df['clean_text'].notna() & (df['clean_text'] != '')]

    # Stage 2: NLP extraction
    if progress_callback:
        progress_callback(0.15, "Extracting NLP components...")
    nlp_results = []
    for text in df['clean_text']:
        result = extract_enhanced_roles(text, nlp)
        nlp_results.append(result)

    for key in ['action_verb', 'subject', 'verb', 'object', 'constraint']:
        df[key] = [result[key] for result in nlp_results]

    df = df[df['action_verb'].notna() & (df['action_verb'] != '')]
    df.reset_index(drop=True, inplace=True)

    if use_parallel:
        # HYBRID MODE: Parallel for refinement and constraints, sequential for INCOSE
        batch_size = 8
        max_workers = 5

        # Stage 3: Refinement (parallel)
        if progress_callback:
            progress_callback(0.25, "Refining components (parallel)...")

        refinement_data = []
        for i, row in df.iterrows():
            refinement_data.append((i, getattr(row, 'subject', None), getattr(row, 'object', None), getattr(row, 'verb', None), getattr(row, 'clean_text', '')))

        batches = [refinement_data[i:i + batch_size] for i in range(0, len(refinement_data), batch_size)]
        refined_results = [None] * len(df)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_batch = {executor.submit(process_refinement_batch, client, batch): batch for batch in batches}
            completed_batches = 0
            for future in as_completed(future_to_batch):
                batch_results = future.result()
                for idx, result in batch_results:
                    refined_results[idx] = result
                completed_batches += 1
                if progress_callback:
                    progress_callback(0.25 + (completed_batches / len(batches)) * 0.2, f"Refinement: {completed_batches}/{len(batches)}")

        # Stage 4: Constraints (parallel)
        if progress_callback:
            progress_callback(0.5, "Identifying constraints (parallel)...")

        constraint_data = [(i, text) for i, text in enumerate(df['clean_text'])]
        constraint_batches = [constraint_data[i:i + batch_size] for i in range(0, len(constraint_data), batch_size)]
        constraint_results = [None] * len(df)

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_batch = {executor.submit(process_constraint_batch, client, batch): batch for batch in constraint_batches}
            completed_batches = 0
            for future in as_completed(future_to_batch):
                batch_results = future.result()
                for idx, result in batch_results:
                    constraint_results[idx] = result
                completed_batches += 1
                if progress_callback:
                    progress_callback(0.5 + (completed_batches / len(constraint_batches)) * 0.2, f"Constraints: {completed_batches}/{len(constraint_batches)}")

        # Stage 5: INCOSE compliance (ALWAYS SEQUENTIAL)
        if progress_callback:
            progress_callback(0.75, "INCOSE compliance analysis (sequential)...")

        incose_results = []
        for idx, row in df.iterrows():
            try:
                text = row.get('clean_text', '')
                result = incose_compliance_with_gpt(client, text)
                incose_results.append(result)
                if progress_callback and idx % 5 == 0:
                    progress_callback(0.75 + (idx / len(df)) * 0.2, f"INCOSE sequential: {idx+1}/{len(df)}")
                time.sleep(0.1)  # Rate limiting for sequential processing
            except Exception as e:
                incose_results.append({"clarity_level": "Unknown", "issues": [f"Analysis failed: {str(e)}"], "likely_violated_rules": [], "improved_requirement": ""})

    else:
        # FULL SEQUENTIAL MODE: Everything runs sequentially
        if progress_callback:
            progress_callback(0.3, "Refining components (sequential)...")
        refined_results = []
        for idx, row in df.iterrows():
            try:
                subject = row.get('subject', None)
                obj = row.get('object', None)
                verb = row.get('verb', None)
                text = row.get('clean_text', '')
                result = refine_with_descriptors(client, subject, obj, verb, text)
                refined_results.append(result)
                if progress_callback and idx % 5 == 0:
                    progress_callback(0.3 + (idx / len(df)) * 0.15, f"Refining: {idx+1}/{len(df)}")
                time.sleep(0.1)
            except Exception as e:
                refined_results.append({"subject_refined": None, "verb_refined": None, "object_refined": None, "confidence_score": 0.0, "notes": f"Error: {str(e)}"})

        if progress_callback:
            progress_callback(0.5, "Identifying constraints (sequential)...")
        constraint_results = []
        for idx, row in df.iterrows():
            try:
                text = row.get('clean_text', '')
                result = identify_constraints_with_chatgpt(client, text)
                constraint_results.append(result)
                if progress_callback and idx % 5 == 0:
                    progress_callback(0.5 + (idx / len(df)) * 0.15, f"Constraints: {idx+1}/{len(df)}")
                time.sleep(0.1)
            except Exception as e:
                constraint_results.append({"constraints_identified": [], "constraint_types": [], "constraint_values": [], "constraint_count": 0, "confidence_score": 0.0, "notes": f"Error: {str(e)}"})

        if progress_callback:
            progress_callback(0.7, "INCOSE compliance analysis (sequential)...")
        incose_results = []
        for idx, row in df.iterrows():
            try:
                text = row.get('clean_text', '')
                result = incose_compliance_with_gpt(client, text)
                incose_results.append(result)
                if progress_callback and idx % 5 == 0:
                    progress_callback(0.7 + (idx / len(df)) * 0.25, f"INCOSE sequential: {idx+1}/{len(df)}")
                time.sleep(0.1)
            except Exception as e:
                incose_results.append({"clarity_level": "Unknown", "issues": [f"Analysis failed: {str(e)}"], "likely_violated_rules": [], "improved_requirement": ""})

    # Add results to dataframe
    if progress_callback:
        progress_callback(0.95, "Finalizing results...")

    df['subject_refined'] = [r.get('subject_refined') if r else None for r in refined_results]
    df['object_refined'] = [r.get('object_refined') if r else None for r in refined_results]
    df['verb_refined'] = [r.get('verb_refined') if r else None for r in refined_results]
    df['refinement_confidence'] = [r.get('confidence_score', 0.0) if r else 0.0 for r in refined_results]
    df['refinement_notes'] = [r.get('notes', '') if r else '' for r in refined_results]

    df['constraints_identified'] = [r.get('constraints_identified', []) if r else [] for r in constraint_results]
    df['constraint_types'] = [r.get('constraint_types', []) if r else [] for r in constraint_results]
    df['constraint_values'] = [r.get('constraint_values', []) if r else [] for r in constraint_results]
    df['constraint_count'] = [r.get('constraint_count', 0) if r else 0 for r in constraint_results]
    df['constraint_confidence'] = [r.get('confidence_score', 0.0) if r else 0.0 for r in constraint_results]
    df['constraint_notes'] = [r.get('notes', '') if r else '' for r in constraint_results]

    df['clarity_level'] = [r.get('clarity_level', 'Unknown') if r else 'Unknown' for r in incose_results]
    score_mapping = {"High": 90, "Medium": 70, "Low": 50, "Unknown": 0}
    df['clarity_score'] = [score_mapping.get(r.get('clarity_level', 'Unknown'), 0) if r else 0 for r in incose_results]
    df['gpt_issues'] = ['; '.join(r.get('issues', [])) if r and r.get('issues') else 'No issues identified' for r in incose_results]
    df['gpt_violated_rules'] = ['; '.join(r.get('likely_violated_rules', [])) if r and r.get('likely_violated_rules') else 'No rules violated' for r in incose_results]
    df['gpt_suggestion'] = [r.get('improved_requirement', 'No improvement suggested') if r else 'No improvement suggested' for r in incose_results]

    df['discipline'] = discipline
    df['document_type'] = document_type
    df['potential_variables'] = ''
    df['final_clause_after_sme_review'] = ''

    if progress_callback:
        progress_callback(1.0, "Processing complete!")

    return df

# Output preparation
def prepare_output_dataframe(df):
    output_columns = {
        'clean_text': 'Requirements',
        'subject_refined': 'Subject',
        'object_refined': 'Object',
        'verb_refined': 'Action Verb',
        'refinement_notes': 'Refinement Notes',
        'potential_variables': 'Potential Variables (Project Specifics)',
        'constraints_identified': 'constraints_identified',
        'constraint_types': 'constraint_types',
        'constraint_values': 'constraint_values',
        'constraint_notes': 'constraint_notes',
        'clarity_level': 'Level of Clarity',
        'clarity_score': 'Clarity Score',
        'gpt_issues': 'Issues Found',
        'gpt_violated_rules': 'INCOSE Violated Rules',
        'gpt_suggestion': 'Suggestions to Improve',
        'final_clause_after_sme_review': 'Final Clause After Reviewed by SME',
        'discipline': 'Discipline',
        'document_type': 'Document Type'
    }

    output_df = pd.DataFrame()
    for original_col, new_col in output_columns.items():
        if original_col in df.columns:
            output_df[new_col] = df[original_col]
        else:
            output_df[new_col] = ''
    return output_df

# PDF creation function with filename
def create_comprehensive_pdf_report(df, discipline, document_type, filename):
    if not PDF_AVAILABLE:
        return None

    try:
        buffer = io.BytesIO()
        doc = SimpleDocTemplate(buffer, pagesize=A4, rightMargin=72, leftMargin=72, topMargin=72, bottomMargin=18)
        styles = getSampleStyleSheet()
        story = []

        # Title
        title = Paragraph("NLP Requirements Analysis Report", styles['Title'])
        story.append(title)
        story.append(Spacer(1, 12))

        # Project info
        project_info = f"""
        <b>Project Configuration:</b><br/>
        Document: {filename}<br/>
        Discipline: {discipline}<br/>
        Document Type: {document_type}<br/>
        Analysis Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}<br/>
        Total Requirements: {len(df)}
        """
        story.append(Paragraph(project_info, styles['Normal']))
        story.append(Spacer(1, 20))

        # Summary metrics
        high_clarity = len(df[df['clarity_level'] == 'High'])
        medium_clarity = len(df[df['clarity_level'] == 'Medium'])
        low_clarity = len(df[df['clarity_level'] == 'Low'])
        with_constraints = len(df[df['constraint_count'] > 0])
        avg_confidence = df['refinement_confidence'].mean()

        summary = f"""
        <b>Analysis Summary:</b><br/>
        ‚Ä¢ High Clarity: {high_clarity} ({high_clarity/len(df)*100:.1f}%)<br/>
        ‚Ä¢ Medium Clarity: {medium_clarity} ({medium_clarity/len(df)*100:.1f}%)<br/>
        ‚Ä¢ Low Clarity: {low_clarity} ({low_clarity/len(df)*100:.1f}%)<br/>
        ‚Ä¢ Requirements with Constraints: {with_constraints}<br/>
        ‚Ä¢ Average Confidence: {avg_confidence:.2f}
        """
        story.append(Paragraph(summary, styles['Normal']))
        story.append(PageBreak())

        # Generate charts
        try:
            fig1, fig2, fig3, fig4 = create_summary_charts(df)

            charts = [
                (fig1, "INCOSE Clarity Level Distribution"),
                (fig2, "Refinement Confidence Distribution"),
                (fig3, "Number of Constraints per Requirement"),
                (fig4, "Top 10 Most Common Action Verbs")
            ]

            for fig, title in charts:
                story.append(Paragraph(f"<b>{title}</b>", styles['Heading2']))
                story.append(Spacer(1, 12))

                try:
                    img_bytes = pio.to_image(fig, format="png", width=800, height=600, scale=1)
                    img_buffer = io.BytesIO(img_bytes)
                    img = Image(img_buffer, width=6*inch, height=4.5*inch)
                    story.append(img)
                    story.append(Spacer(1, 20))

                except Exception as chart_error:
                    error_text = f"Chart generation failed: {str(chart_error)}"
                    story.append(Paragraph(error_text, styles['Normal']))
                    story.append(Spacer(1, 20))

        except Exception as e:
            story.append(Paragraph(f"Error generating charts: {str(e)}", styles['Normal']))

        doc.build(story)
        buffer.seek(0)
        return buffer

    except Exception as e:
        print(f"PDF creation failed: {e}")
        return None

# Charts
def create_summary_charts(df):
    # Clarity distribution
    clarity_counts = df['clarity_level'].value_counts()
    fig1 = px.pie(
        values=clarity_counts.values,
        names=clarity_counts.index,
        title="INCOSE Clarity Level Distribution",
        color_discrete_map={"High": "#2ecc71", "Medium": "#f39c12", "Low": "#e74c3c", "Unknown": "#95a5a6"}
    )

    # Confidence distribution
    fig2 = px.histogram(
        df,
        x='refinement_confidence',
        nbins=20,
        title="Refinement Confidence Distribution"
    )

    # Constraint count
    constraint_counts = df['constraint_count'].value_counts().sort_index()
    fig3 = px.bar(
        x=constraint_counts.index,
        y=constraint_counts.values,
        title="Constraints per Requirement"
    )

    # Action verbs
    action_verb_counts = df['action_verb'].value_counts().head(10)
    fig4 = px.bar(
        x=action_verb_counts.values,
        y=action_verb_counts.index,
        orientation='h',
        title="Top 10 Action Verbs"
    )

    return fig1, fig2, fig3, fig4

# Main Streamlit App
def main():
    st.markdown('<h1 class="main-header">üî¨ NLP Requirements Analysis Tool</h1>', unsafe_allow_html=True)

    st.markdown("""
    This tool processes engineering requirements through comprehensive NLP analysis including:
    - **Text Cleaning & Preprocessing**
    - **Subject-Verb-Object Extraction**
    - **Component Refinement with ChatGPT (Parallel)**
    - **Constraint Identification (Parallel)**
    - **INCOSE Compliance Assessment (Sequential)**
    """)

    # Sidebar configuration
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")

        # API Key
        api_key = st.text_input(
            "OpenAI API Key",
            type="password",
            help="Enter your OpenAI API key for ChatGPT processing"
        )

        if not api_key:
            st.warning("Please enter your OpenAI API key to proceed")

        st.markdown("---")

        # File upload
        uploaded_file = st.file_uploader(
            "Upload File",
            type=['csv', 'xlsx', 'xls'],
            help="Upload a CSV or Excel file with requirements. The tool will automatically detect 'Numbering' and 'Text' columns from any position."
        )

        # Store filename and show column detection info
        if uploaded_file:
            st.session_state.uploaded_filename = get_filename_without_extension(uploaded_file.name)
            st.info(f"üìÑ **File:** {uploaded_file.name}")

            # Perform smart column detection for preview
            df_preview, detection_info, error_msg = prepare_dataframe_with_smart_detection(uploaded_file)

            if error_msg:
                st.error(f"‚ùå {error_msg}")
            elif detection_info:
                if detection_info['method'] == 'smart_detection':
                    st.success(f"‚úÖ **Smart Detection:** Found '{detection_info['numbering_col']}' and '{detection_info['text_col']}'")
                elif detection_info['method'] == 'fallback_position':
                    st.warning(f"‚ö†Ô∏è **Fallback Mode:** {detection_info.get('warning', 'Using first two columns')}")

                # Show all available columns
                with st.expander("üìã View All Available Columns"):
                    st.write("**Available columns in your file:**")
                    for i, col in enumerate(detection_info['all_columns'], 1):
                        status = ""
                        if col == detection_info.get('numbering_col'):
                            status = " ‚Üê **Selected as Numbering**"
                        elif col == detection_info.get('text_col'):
                            status = " ‚Üê **Selected as Text**"
                        st.write(f"{i}. {col}{status}")

            st.info(f"üîß **Processing:** Smart column detection from {detection_info['total_columns'] if detection_info else 'unknown'} columns")

        # Project configuration
        if uploaded_file:
            st.markdown("---")
            st.header("üìã Project Configuration")

            # Discipline
            discipline_options = ["Mechanical", "Electrical", "Instrument", "Process", "Other (Specify)"]
            selected_discipline_option = st.selectbox(
                "Select Discipline",
                discipline_options,
                index=0,
                help="Choose the engineering discipline or select 'Other' to specify custom"
            )

            if selected_discipline_option == "Other (Specify)":
                custom_discipline = st.text_input(
                    "Specify Discipline",
                    placeholder="Enter your discipline (e.g., Civil, Chemical, Software, etc.)",
                    help="Enter the specific discipline for your project"
                )
                st.session_state.selected_discipline = custom_discipline if custom_discipline else "Custom Discipline"
            else:
                st.session_state.selected_discipline = selected_discipline_option

            # Document type
            document_type_options = ["Design Basis", "Functional Requirements", "Technical Specifications", "System Requirements", "Other (Specify)"]
            selected_document_type_option = st.selectbox(
                "Select Document Type",
                document_type_options,
                index=0,
                help="Choose the document type or select 'Other' to specify custom"
            )

            if selected_document_type_option == "Other (Specify)":
                custom_document_type = st.text_input(
                    "Specify Document Type",
                    placeholder="Enter your document type (e.g., User Stories, Acceptance Criteria, etc.)",
                    help="Enter the specific document type for your project"
                )
                st.session_state.selected_document_type = custom_document_type if custom_document_type else "Custom Document Type"
            else:
                st.session_state.selected_document_type = selected_document_type_option

            st.markdown("---")
            st.header("‚ö° Performance Settings")

            # Updated speed option info
            speed_option = st.radio(
                "Processing Mode",
                ["üöÄ Hybrid Processing (Recommended)", "üêå Full Sequential"],
                index=0,
                help="Hybrid: Parallel for refinement & constraints, sequential for INCOSE rules analysis"
            )
            st.session_state.speed_option = speed_option

            if "Hybrid" in speed_option:
                st.success("‚úÖ Hybrid mode: Parallel processing for refinement & constraints, sequential for INCOSE compliance")
            else:
                st.info("‚ÑπÔ∏è Full sequential mode - All processes run sequentially with rate limiting")

        # Start processing button
        if uploaded_file and api_key and st.session_state.selected_discipline and st.session_state.selected_document_type:
            # Check if custom inputs are provided when "Other" is selected
            discipline_ready = (st.session_state.selected_discipline != "Other (Specify)" and
                              st.session_state.selected_discipline and
                              st.session_state.selected_discipline != "Custom Discipline") or \
                             (st.session_state.selected_discipline and st.session_state.selected_discipline != "")

            document_type_ready = (st.session_state.selected_document_type != "Other (Specify)" and
                                 st.session_state.selected_document_type and
                                 st.session_state.selected_document_type != "Custom Document Type") or \
                                (st.session_state.selected_document_type and st.session_state.selected_document_type != "")

            if discipline_ready and document_type_ready:
                if st.button("üöÄ Start Processing", type="primary"):
                    st.session_state.start_processing = True
            else:
                st.warning("‚ö†Ô∏è Please specify discipline and document type if 'Other' is selected")

        st.markdown("---")

        # Export options (show only if processing complete)
        if st.session_state.processing_complete and st.session_state.processed_df is not None:
            st.header("üì• Export Options")
            filename = st.session_state.uploaded_filename or "nlp_analysis"

            # CSV Download
            output_df = prepare_output_dataframe(st.session_state.processed_df)
            csv = output_df.to_csv(index=False)
            st.download_button(
                label="üìä Download Complete Results (CSV)",
                data=csv,
                file_name=f"{filename}_complete_analysis.csv",
                mime="text/csv",
                use_container_width=True,
                key="sidebar_csv_download"
            )

            # PDF Download
            if PDF_AVAILABLE:
                try:
                    pdf_buffer = create_comprehensive_pdf_report(
                        st.session_state.processed_df,
                        st.session_state.selected_discipline,
                        st.session_state.selected_document_type,
                        filename
                    )
                    if pdf_buffer:
                        st.download_button(
                            label="üìÑ Download Analysis Report (PDF)",
                            data=pdf_buffer.getvalue(),
                            file_name=f"{filename}_analysis_report.pdf",
                            mime="application/pdf",
                            use_container_width=True,
                            key="sidebar_pdf_download"
                        )
                except Exception as e:
                    st.error(f"PDF generation error: {str(e)}")
            else:
                st.info("üìÑ Install reportlab for PDF export: pip install reportlab")

            # Summary CSV
            summary_data = {
                "document_name": filename,
                "total_requirements": len(st.session_state.processed_df),
                "discipline": st.session_state.selected_discipline,
                "document_type": st.session_state.selected_document_type,
                "high_clarity_count": len(st.session_state.processed_df[st.session_state.processed_df['clarity_level'] == 'High']),
                "medium_clarity_count": len(st.session_state.processed_df[st.session_state.processed_df['clarity_level'] == 'Medium']),
                "low_clarity_count": len(st.session_state.processed_df[st.session_state.processed_df['clarity_level'] == 'Low']),
                "average_refinement_confidence": st.session_state.processed_df['refinement_confidence'].mean(),
                "requirements_with_constraints": len(st.session_state.processed_df[st.session_state.processed_df['constraint_count'] > 0]),
                "average_constraints_per_requirement": st.session_state.processed_df['constraint_count'].mean(),
                "most_common_action_verb": st.session_state.processed_df['action_verb'].mode().iloc[0] if not st.session_state.processed_df['action_verb'].mode().empty else "N/A"
            }
            summary_df = pd.DataFrame([summary_data])
            summary_csv = summary_df.to_csv(index=False)
            st.download_button(
                label="üìã Download Summary (CSV)",
                data=summary_csv,
                file_name=f"{filename}_summary.csv",
                mime="text/csv",
                use_container_width=True,
                key="sidebar_summary_download"
            )

    # Main content area
    if uploaded_file is None:
        st.info("üëÜ Please upload a CSV or Excel file to begin analysis")

        st.markdown('<h2 class="section-header">üìã Expected Data Format</h2>', unsafe_allow_html=True)
        sample_data = pd.DataFrame({
            'Numbering': ['1', '2', '3'],
            'Requirements Text': [
                'The system shall automatically shut down within 5 seconds when temperature exceeds 80¬∞C',
                'The valve must open completely in less than 2 minutes under normal operating conditions',
                'Each sensor should provide continuous monitoring of pressure levels'
            ]
        })
        st.dataframe(sample_data, use_container_width=True)
        st.caption("The tool will automatically detect 'Numbering' and 'Text' columns regardless of their position or exact names.")

        st.markdown("### üéØ Smart Column Detection")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("""
            **‚úÖ Numbering Column Patterns:**
            - Numbering, Number, Num, ID
            - Req ID, Item, Index, Sr No
            - Serial, Sequence, Ref
            """)
        with col2:
            st.markdown("""
            **‚úÖ Text Column Patterns:**
            - Text, Requirement, Requirements
            - Description, Requirement Text
            - Details, Specification, Content
            """)

        st.markdown("### üìÅ Supported File Formats")
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("""
            **‚úÖ CSV Files (.csv)**
            - Automatic encoding detection
            - Smart column detection
            """)
        with col2:
            st.markdown("""
            **‚úÖ Excel Files (.xlsx, .xls)**
            - First sheet automatically selected
            - Smart column detection
            """)

    elif not api_key:
        st.warning("‚ö†Ô∏è Please enter your OpenAI API key in the sidebar")

    elif not st.session_state.selected_discipline or not st.session_state.selected_document_type:
        st.info("üìã Please select discipline and document type in the sidebar")

        # Show current selection status
        if st.session_state.selected_discipline:
            if st.session_state.selected_discipline in ["Custom Discipline", ""]:
                st.warning("‚ö†Ô∏è Please specify your custom discipline")
            else:
                st.success(f"‚úÖ Discipline selected: {st.session_state.selected_discipline}")

        if st.session_state.selected_document_type:
            if st.session_state.selected_document_type in ["Custom Document Type", ""]:
                st.warning("‚ö†Ô∏è Please specify your custom document type")
            else:
                st.success(f"‚úÖ Document type selected: {st.session_state.selected_document_type}")

    else:
        # File processing with smart column detection
        try:
            # Read file using smart detection
            df, detection_info, error_msg = prepare_dataframe_with_smart_detection(uploaded_file)

            if error_msg:
                st.error(f"‚ùå {error_msg}")
                return

            if df is None:
                st.error("‚ùå Could not process the file")
                return

            st.session_state.original_df = df

            # Show success message with detection details
            if detection_info['method'] == 'smart_detection':
                st.success(f"‚úÖ File processed successfully! Smart detection found '{detection_info['numbering_col']}' and '{detection_info['text_col']}' columns from {detection_info['total_columns']} total columns.")
            else:
                st.success(f"‚úÖ File processed successfully! Used fallback method with first two columns from {detection_info['total_columns']} total columns.")
                if detection_info.get('warning'):
                    st.warning(f"‚ö†Ô∏è {detection_info['warning']}")

            # Show what columns were processed
            col1, col2 = st.columns(2)
            with col1:
                numbering_col_name = detection_info.get('numbering_col', df.columns[0])
                st.info(f"**Numbering Column:** {numbering_col_name}")
            with col2:
                text_col_name = detection_info.get('text_col', df.columns[1])
                st.info(f"**Text Column:** {text_col_name}")

            # Show configuration
            col1, col2, col3 = st.columns(3)
            with col1:
                st.info(f"**Discipline:** {st.session_state.selected_discipline}")
            with col2:
                st.info(f"**Document Type:** {st.session_state.selected_document_type}")
            with col3:
                processing_mode = "Hybrid" if "Hybrid" in str(st.session_state.speed_option) else "Sequential"
                st.info(f"**Processing:** {processing_mode}")

            # Data preview
            st.markdown('<h2 class="section-header">üëÄ Data Preview</h2>', unsafe_allow_html=True)

            # Show column detection summary
            with st.expander("üîç Column Detection Details"):
                st.write(f"**Detection Method:** {detection_info['method'].replace('_', ' ').title()}")
                st.write(f"**File Type:** {detection_info['file_type']}")
                st.write(f"**Total Columns Available:** {detection_info['total_columns']}")
                st.write("**All Available Columns:**")
                for i, col in enumerate(detection_info['all_columns'], 1):
                    status = ""
                    if col == detection_info.get('numbering_col'):
                        status = " ‚Üê **Used as Numbering**"
                    elif col == detection_info.get('text_col'):
                        status = " ‚Üê **Used as Text**"
                    st.write(f"   {i}. {col}{status}")

            st.dataframe(df.head(10), use_container_width=True)

            # Process if button clicked
            if hasattr(st.session_state, 'start_processing') and st.session_state.start_processing:
                st.session_state.start_processing = False  # Reset flag

                st.markdown('<h2 class="section-header">üîÑ Processing Status</h2>', unsafe_allow_html=True)

                # Progress tracking
                progress_bar = st.progress(0)
                status_text = st.empty()

                with st.spinner("Processing requirements..."):
                    def update_progress(progress, message):
                        progress_bar.progress(progress)
                        status_text.text(message)

                    try:
                        use_parallel = "Hybrid" in str(st.session_state.speed_option)
                        processed_df = process_dataframe(
                            df,
                            api_key,
                            st.session_state.selected_discipline,
                            st.session_state.selected_document_type,
                            update_progress,
                            use_parallel
                        )

                        if processed_df is not None:
                            st.session_state.processed_df = processed_df
                            st.session_state.processing_complete = True
                            processing_mode = "hybrid (parallel + sequential)" if use_parallel else "sequential"
                            st.success(f"üéâ Processing completed successfully using {processing_mode} processing!")
                        else:
                            st.error("‚ùå Processing failed. Please check your API key and try again.")

                    except Exception as e:
                        st.error(f"‚ùå An error occurred during processing: {str(e)}")
                        st.session_state.processing_complete = False

        except Exception as e:
            st.error(f"‚ùå Error reading file: {str(e)}")
            st.info("üí° For CSV files, try different encoding. For Excel files, check file format.")

    # Show results if processing complete
    if st.session_state.processing_complete and st.session_state.processed_df is not None:
        df_processed = st.session_state.processed_df
        filename = st.session_state.uploaded_filename or "nlp_analysis"

        st.markdown('<h2 class="section-header">üìä Analysis Results</h2>', unsafe_allow_html=True)

        # Quick downloads at top
        st.markdown("### üì• Quick Downloads")
        col1, col2, col3 = st.columns(3)

        with col1:
            output_df = prepare_output_dataframe(df_processed)
            csv = output_df.to_csv(index=False)
            st.download_button(
                label="üìä Download Complete Results (CSV)",
                data=csv,
                file_name=f"{filename}_complete_analysis.csv",
                mime="text/csv",
                use_container_width=True,
                key="main_csv_download"
            )

        with col2:
            if PDF_AVAILABLE:
                try:
                    pdf_buffer = create_comprehensive_pdf_report(
                        df_processed,
                        st.session_state.selected_discipline,
                        st.session_state.selected_document_type,
                        filename
                    )
                    if pdf_buffer:
                        st.download_button(
                            label="üìÑ Download Analysis Report (PDF)",
                            data=pdf_buffer.getvalue(),
                            file_name=f"{filename}_analysis_report.pdf",
                            mime="application/pdf",
                            use_container_width=True,
                            key="main_pdf_download"
                        )
                except Exception as e:
                    st.error(f"PDF error: {str(e)}")
            else:
                st.info("Install reportlab for PDF")

        with col3:
            summary_data = {
                "document_name": filename,
                "total_requirements": len(df_processed),
                "discipline": st.session_state.selected_discipline,
                "document_type": st.session_state.selected_document_type,
                "high_clarity_count": len(df_processed[df_processed['clarity_level'] == 'High']),
                "medium_clarity_count": len(df_processed[df_processed['clarity_level'] == 'Medium']),
                "low_clarity_count": len(df_processed[df_processed['clarity_level'] == 'Low']),
                "average_refinement_confidence": df_processed['refinement_confidence'].mean(),
                "requirements_with_constraints": len(df_processed[df_processed['constraint_count'] > 0]),
                "average_constraints_per_requirement": df_processed['constraint_count'].mean(),
                "most_common_action_verb": df_processed['action_verb'].mode().iloc[0] if not df_processed['action_verb'].mode().empty else "N/A"
            }
            summary_df = pd.DataFrame([summary_data])
            summary_csv = summary_df.to_csv(index=False)
            st.download_button(
                label="üìã Download Summary (CSV)",
                data=summary_csv,
                file_name=f"{filename}_summary.csv",
                mime="text/csv",
                use_container_width=True,
                key="main_summary_download"
            )

        st.markdown("---")

        # Results tabs
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["üìà Summary", "üîç Detailed Results", "üìã Components", "‚öñÔ∏è Constraints", "‚úÖ INCOSE Compliance"])

        with tab1:
            st.markdown("### üìä Processing Summary")

            # Key metrics
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric(
                    "Total Requirements",
                    len(df_processed),
                    delta=f"From {len(st.session_state.original_df)} original"
                )

            with col2:
                high_clarity = len(df_processed[df_processed['clarity_level'] == 'High'])
                st.metric(
                    "High Clarity",
                    high_clarity,
                    delta=f"{high_clarity/len(df_processed)*100:.1f}%"
                )

            with col3:
                with_constraints = len(df_processed[df_processed['constraint_count'] > 0])
                st.metric(
                    "With Constraints",
                    with_constraints,
                    delta=f"{with_constraints/len(df_processed)*100:.1f}%"
                )

            with col4:
                avg_confidence = df_processed['refinement_confidence'].mean()
                st.metric(
                    "Avg Confidence",
                    f"{avg_confidence:.2f}",
                    delta="Refinement quality"
                )

            # Configuration info
            st.markdown("### üìã Project Configuration")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.info(f"**Document:** {filename}")
            with col2:
                st.info(f"**Discipline:** {st.session_state.selected_discipline}")
            with col3:
                st.info(f"**Document Type:** {st.session_state.selected_document_type}")
            with col4:
                processing_mode = "Hybrid" if "Hybrid" in str(st.session_state.speed_option) else "Sequential"
                st.info(f"**Processing:** {processing_mode}")

            # Processing method explanation
            if "Hybrid" in str(st.session_state.speed_option):
                st.success("üîÑ **Processing Method:** Hybrid - Parallel processing for refinement & constraints, sequential for INCOSE compliance analysis")
            else:
                st.info("üîÑ **Processing Method:** Full Sequential - All analyses performed sequentially")

            # Charts
            st.markdown("### üìà Visual Analysis")
            fig1, fig2, fig3, fig4 = create_summary_charts(df_processed)

            col1, col2 = st.columns(2)
            with col1:
                st.plotly_chart(fig1, use_container_width=True)
                st.plotly_chart(fig3, use_container_width=True)

            with col2:
                st.plotly_chart(fig2, use_container_width=True)
                st.plotly_chart(fig4, use_container_width=True)

        with tab2:
            st.markdown("### üîç Complete Analysis Results")

            # Filters
            col1, col2, col3 = st.columns(3)

            with col1:
                clarity_filter = st.selectbox(
                    "Filter by Clarity Level",
                    ["All"] + list(df_processed['clarity_level'].unique())
                )

            with col2:
                min_confidence = st.slider(
                    "Minimum Refinement Confidence",
                    0.0, 1.0, 0.0, 0.1
                )

            with col3:
                show_constraints_only = st.checkbox("Show only requirements with constraints")

            # Apply filters
            filtered_df = df_processed.copy()

            if clarity_filter != "All":
                filtered_df = filtered_df[filtered_df['clarity_level'] == clarity_filter]

            filtered_df = filtered_df[filtered_df['refinement_confidence'] >= min_confidence]

            if show_constraints_only:
                filtered_df = filtered_df[filtered_df['constraint_count'] > 0]

            st.info(f"Showing {len(filtered_df)} of {len(df_processed)} requirements")

            # Display results
            display_df = prepare_output_dataframe(filtered_df)
            st.dataframe(
                display_df[[
                    'Requirements', 'Action Verb', 'Subject', 'Object',
                    'Level of Clarity', 'Clarity Score', 'constraints_identified'
                ]].head(50),
                use_container_width=True
            )

            if len(filtered_df) > 50:
                st.info("Showing first 50 results. Download full CSV for complete data.")

        with tab3:
            st.markdown("### üìã Extracted Components Analysis")

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("#### üéØ Most Common Subjects")
                subjects = df_processed['subject_refined'].dropna()
                if len(subjects) > 0:
                    subject_counts = subjects.value_counts().head(10)
                    st.bar_chart(subject_counts)
                else:
                    st.info("No subjects found")

                st.markdown("#### üîß Most Common Objects")
                objects = df_processed['object_refined'].dropna()
                if len(objects) > 0:
                    object_counts = objects.value_counts().head(10)
                    st.bar_chart(object_counts)
                else:
                    st.info("No objects found")

            with col2:
                st.markdown("#### ‚ö° Most Common Verbs")
                verbs = df_processed['verb_refined'].dropna()
                if len(verbs) > 0:
                    verb_counts = verbs.value_counts().head(10)
                    st.bar_chart(verb_counts)
                else:
                    st.info("No verbs found")

                st.markdown("#### üìè Action Verb Distribution")
                action_verbs = df_processed['action_verb'].value_counts()
                fig = px.pie(values=action_verbs.values, names=action_verbs.index)
                st.plotly_chart(fig, use_container_width=True)

        with tab4:
            st.markdown("### ‚öñÔ∏è Constraints Analysis")

            # Overview
            total_constraints = df_processed['constraint_count'].sum()
            requirements_with_constraints = len(df_processed[df_processed['constraint_count'] > 0])

            col1, col2, col3 = st.columns(3)

            with col1:
                st.metric("Total Constraints Found", total_constraints)

            with col2:
                st.metric("Requirements with Constraints", requirements_with_constraints)

            with col3:
                avg_constraints = df_processed['constraint_count'].mean()
                st.metric("Average per Requirement", f"{avg_constraints:.2f}")

            # Top constrained requirements
            st.markdown("#### üîù Requirements with Most Constraints")
            top_constrained = df_processed.nlargest(10, 'constraint_count')[
                ['clean_text', 'constraint_count', 'constraints_identified']
            ]
            st.dataframe(top_constrained, use_container_width=True)

        with tab5:
            st.markdown("### ‚úÖ INCOSE Compliance Analysis")
            st.info("üîÑ **Note:** INCOSE compliance analysis was performed sequentially for thorough rule evaluation")

            # Overview
            clarity_counts = df_processed['clarity_level'].value_counts()

            col1, col2, col3, col4 = st.columns(4)

            with col1:
                high_count = clarity_counts.get('High', 0)
                st.metric("High Clarity", high_count, f"{high_count/len(df_processed)*100:.1f}%")

            with col2:
                medium_count = clarity_counts.get('Medium', 0)
                st.metric("Medium Clarity", medium_count, f"{medium_count/len(df_processed)*100:.1f}%")

            with col3:
                low_count = clarity_counts.get('Low', 0)
                st.metric("Low Clarity", low_count, f"{low_count/len(df_processed)*100:.1f}%")

            with col4:
                avg_score = df_processed['clarity_score'].mean()
                st.metric("Average Score", f"{avg_score:.1f}", "Out of 100")

            # Sample improvements
            st.markdown("#### üí° Example Improvements")

            has_suggestions = df_processed[
                (df_processed['gpt_suggestion'].notna()) &
                (df_processed['gpt_suggestion'] != 'No improvement suggested') &
                (df_processed['gpt_suggestion'] != '')
            ]

            if len(has_suggestions) > 0:
                sample_improvements = has_suggestions.head(5)

                for idx, row in sample_improvements.iterrows():
                    with st.expander(f"Improvement Example {idx + 1}"):
                        st.markdown("**Original:**")
                        st.write(row['clean_text'])

                        st.markdown("**Issues:**")
                        st.write(row['gpt_issues'])

                        st.markdown("**Violated Rules:**")
                        st.write(row['gpt_violated_rules'])

                        st.markdown("**Suggested Improvement:**")
                        st.write(row['gpt_suggestion'])

                        st.markdown("**Clarity Level:**")
                        st.write(row['clarity_level'])
            else:
                st.info("No improvement suggestions available")

if __name__ == "__main__":
    main()